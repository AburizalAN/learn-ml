{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AburizalAN/learn-ml/blob/master/langchain/04_memory.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tdiHwcPP-po4"
      },
      "source": [
        "# Memory and LangChain\n",
        "\n",
        "Interacting with language models can be challenging due to their stateless nature, which means they don't remember previous conversations.\n",
        "\n",
        "This could be an issue when building some applications, like a chatbot, where you want to have a consistent conversation flow. In this context, we introduce memory management in LangChain.\n",
        "\n",
        "LangChain provides several types of memory to maintain the conversation context:\n",
        "\n",
        "- `ConversationBufferMemory`\n",
        "- `ConversationBufferWindowMemory`\n",
        "- `ConversationTokenBufferMemory`\n",
        "- `ConversationSummaryBufferMemory`\n",
        "\n",
        "These memories are designed to manage the conversation history and feed it back into the language model to maintain the conversational flow."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain openai -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1XK2JS4iO406",
        "outputId": "b8482d62-f449-4688-fe6c-66d3ab85bf10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m807.5/807.5 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.4/227.4 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m256.9/256.9 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.6/66.6 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.8/77.8 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.5/138.5 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1pOoSt96-po6"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "OPENAI_API_KEY = userdata.get(\"OPENAI_API_KEY\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6W5uWUIY-po9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96b5ed01-5e2d-4f2c-dfb0-fa49f29b9fcc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.chat_models.openai.ChatOpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
            "  warn_deprecated(\n"
          ]
        }
      ],
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "OpenAIModel = \"gpt-4\"\n",
        "llm = ChatOpenAI(model=OpenAIModel, temperature=0.1, openai_api_key=OPENAI_API_KEY)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QeVR_NmK-po9"
      },
      "source": [
        "## ConversationBufferMemory\n",
        "\n",
        "This type of memory simply stores all the terms or utterances from a conversation. When you use a large language model for a chat conversation, each transaction is independent and stateless. A language model appears to have memory because we provide it with the entire conversation context."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3mpmcIi0-po9"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import ConversationChain\n",
        "from langchain.memory import ConversationBufferMemory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MN-Kksvw-po9"
      },
      "outputs": [],
      "source": [
        "memory = ConversationBufferMemory()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hGlFmV3r-po-"
      },
      "outputs": [],
      "source": [
        "conversationUsingBufferMemory = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory = memory,\n",
        "    verbose=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cd05AilA-po-",
        "outputId": "98826609-3041-4c46-84e3-8c538f16b129",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "Human: Hello, My name is John.\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Hello, John! It's a pleasure to meet you. How can I assist you today?\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "conversationUsingBufferMemory.predict(input=\"Hello, My name is John.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "11dT7ogu-po-",
        "outputId": "1ea5fb40-a770-432b-c49c-b0f7c9554a81",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 288
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: Hello, My name is John.\n",
            "AI: Hello, John! It's a pleasure to meet you. How can I assist you today?\n",
            "Human: What is 123 times 3?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'123 times 3 equals 369.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "conversationUsingBufferMemory.predict(input=\"What is 123 times 3?\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversationUsingBufferMemory.predict(input=\"What is the capital of indonesia ?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "id": "oLVoXxvJXSvk",
        "outputId": "bad39b05-b2c9-4f38-df8a-006bd9befc0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: Hello, My name is John.\n",
            "AI: Hello, John! It's a pleasure to meet you. How can I assist you today?\n",
            "Human: What is 123 times 3?\n",
            "AI: 123 times 3 equals 369.\n",
            "Human: What is the capital of indonesia ?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The capital of Indonesia is Jakarta.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-I0i417n-po_",
        "outputId": "c8a22e0e-0dcb-4143-8182-2b3bfaeb6fd2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: Hello, My name is John.\n",
            "AI: Hello, John! It's a pleasure to meet you. How can I assist you today?\n",
            "Human: What is 123 times 3?\n",
            "AI: 123 times 3 equals 369.\n",
            "Human: What is the capital of indonesia ?\n",
            "AI: The capital of Indonesia is Jakarta.\n",
            "Human: Do you know my name?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Yes, you mentioned earlier that your name is John.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "conversationUsingBufferMemory.predict(input=\"Do you know my name?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mefM8_no-po_",
        "outputId": "289e6f94-7285-41b2-fda9-3576a251cc08",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Human: Hello, My name is John.\n",
            "AI: Hello, John! It's a pleasure to meet you. How can I assist you today?\n",
            "Human: What is 123 times 3?\n",
            "AI: 123 times 3 equals 369.\n",
            "Human: What is the capital of indonesia ?\n",
            "AI: The capital of Indonesia is Jakarta.\n",
            "Human: Do you know my name?\n",
            "AI: Yes, you mentioned earlier that your name is John.\n"
          ]
        }
      ],
      "source": [
        "print(memory.buffer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WE5TNlDN-po_",
        "outputId": "ae28da2c-91b5-4945-ec6b-16b2ab9736fd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': \"Human: Hello, My name is John.\\nAI: Hello, John! It's a pleasure to meet you. How can I assist you today?\\nHuman: What is 123 times 3?\\nAI: 123 times 3 equals 369.\\nHuman: Do you know my name?\\nAI: Yes, John. You mentioned your name earlier in our conversation.\"}"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "memory.load_memory_variables({})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5eMKF98v-po_"
      },
      "outputs": [],
      "source": [
        "memory = ConversationBufferMemory()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ExyEm3QW-ppA"
      },
      "outputs": [],
      "source": [
        "memory.save_context({\"input\": \"Hi\"},\n",
        "                    {\"output\": \"What's up\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s8qYfBB5-ppA",
        "outputId": "e3cdb5d0-261f-4f83-eef9-670d8adac4e4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Human: Hi\n",
            "AI: What's up\n"
          ]
        }
      ],
      "source": [
        "print(memory.buffer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lwALQS41-ppA",
        "outputId": "f259b6ae-e20b-4281-a656-76f48755a9e8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': \"Human: Hi\\nAI: What's up\"}"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "memory.load_memory_variables({})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1RMEcYWI-ppA"
      },
      "outputs": [],
      "source": [
        "memory.save_context({\"input\": \"Not much, just hanging\"},\n",
        "                    {\"output\": \"Cool\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "knKmEcES-ppA",
        "outputId": "ef47cc0f-25a8-47b0-8be1-28e51e6e4734",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': \"Human: Hi\\nAI: What's up\\nHuman: Not much, just hanging\\nAI: Cool\"}"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "memory.load_memory_variables({})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tCOWAIWp-ppA"
      },
      "source": [
        "The **ConversationBufferMemory** is used in scenarios where it's vital for the AI to have full access to the history of an ongoing conversation. It stores all the terms or utterances from a conversation and feeds them into the language model sequentially, allowing it to respond contextually as the conversation progresses.\n",
        "\n",
        "Here are some instances where ConversationBufferMemory can be particularly useful:\n",
        "\n",
        "1. **Chatbots**: In an interactive chatbot application where maintaining continuity and context is critical to the flow of the conversation, ConversationBufferMemory can play a crucial role. The users can ask multiple questions and each answer can depend deeply on the previous exchanges.\n",
        "\n",
        "2. **Customer Support Systems**: In applications like automated customer service or support, ConversationBufferMemory helps keep track of the customer's entire interaction with the AI, leading to a more personalized and effective assistance.\n",
        "\n",
        "3. **Interactive Games**: In AI-driven interactive games where each user input can change the course of the game, this type of memory would be beneficial.\n",
        "\n",
        "In any circumstance where preserving the full conversation context is important for maintaining accurate and meaningful interactions, ConversationBufferMemory provides an essential solution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZX8jnBz-ppB"
      },
      "source": [
        "## ConversationBufferWindowMemory\n",
        "\n",
        "This memory limits the stored conversation to a fixed number of exchanges. For example, you can set it to remember only the last utterance or more. This prevents the memory from growing without control as the conversation gets longer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JkSDFCYF-ppB"
      },
      "outputs": [],
      "source": [
        "from langchain.memory import ConversationBufferWindowMemory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AVmyqdOH-ppB"
      },
      "outputs": [],
      "source": [
        "memory = ConversationBufferWindowMemory(k=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FwbsrMTT-ppB"
      },
      "outputs": [],
      "source": [
        "memory.save_context({\"input\": \"Hi\"},\n",
        "                    {\"output\": \"What's up\"})\n",
        "\n",
        "\n",
        "memory.save_context({\"input\": \"Not much, just hanging\"},\n",
        "                    {\"output\": \"Cool\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hgDdAtVt-ppB",
        "outputId": "a2d8fc50-b0f5-4e95-9aa8-0f8f19be0231",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': \"Human: Hello, My name is John.\\nAI: Hello, John! It's a pleasure to meet you. How can I assist you today?\\nHuman: What is 123 times 3?\\nAI: 123 times 3 equals 369.\\nHuman: What is the capital of indonesia ?\\nAI: The capital of Indonesia is Jakarta.\\nHuman: Do you know my name?\\nAI: Yes, you mentioned earlier that your name is John.\\nHuman: Hi\\nAI: What's up\\nHuman: Not much, just hanging\\nAI: Cool\"}"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "memory.load_memory_variables({})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-F_0RlHu-ppB"
      },
      "outputs": [],
      "source": [
        "memory = ConversationBufferWindowMemory(k=1)\n",
        "\n",
        "conversationUsingBufferWindowMemory = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory = memory,\n",
        "    verbose=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DlUHcRtu-ppC",
        "outputId": "587f8089-113d-41c6-a65b-b0a8310dda97",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "Human: Hello, My name is John.\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Hello, John! It's a pleasure to meet you. How can I assist you today?\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "conversationUsingBufferWindowMemory.predict(input=\"Hello, My name is John.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N7zX_s1u-ppC",
        "outputId": "c80c19ce-85c3-4b40-b32f-70262208df5d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 288
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: Hello, My name is John.\n",
            "AI: Hello, John! It's a pleasure to meet you. How can I assist you today?\n",
            "Human: What is 123 times 3?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'123 times 3 equals 369.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "conversationUsingBufferWindowMemory.predict(input=\"What is 123 times 3?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "chunk size 500\n",
        "overlap 50"
      ],
      "metadata": {
        "id": "-tXO6fdVesnU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conversationUsingBufferWindowMemory.predict(input=\"What is capital city of malaysia\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 288
        },
        "id": "HxV8cRaqXx7-",
        "outputId": "9b2c8492-3811-435b-cd64-91872065d38b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: What is 123 times 3?\n",
            "AI: 123 times 3 equals 369.\n",
            "Human: What is capital city of malaysia\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The capital city of Malaysia is Kuala Lumpur.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZkbiRSmq-ppC",
        "outputId": "ddf397e1-6140-480b-86c0-d52f58ecea05",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 305
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: What is capital city of malaysia\n",
            "AI: The capital city of Malaysia is Kuala Lumpur.\n",
            "Human: Do you know my name?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"I'm sorry, but I don't know your name. As an AI, I don't have access to personal data about individuals unless it has been shared with me in the course of our conversation. I am designed to respect user privacy and confidentiality.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "conversationUsingBufferWindowMemory.predict(input=\"Do you know my name?\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(memory.buffer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "58fpVHSKX845",
        "outputId": "682888af-0538-4d50-a36e-ceb43a26ce5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Human: Do you know my name?\n",
            "AI: I'm sorry, but I don't know your name. As an AI, I don't have access to personal data about individuals unless it has been shared with me in the course of our conversation. I am designed to respect user privacy and confidentiality.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5oiY5r1-ppC"
      },
      "source": [
        "**ConversationBufferWindowMemory** can be utilized when you want the AI to remember only a limited number of past exchanges. This type of memory grows only up to a certain point, retaining the most recent elements of the conversation and discarding the oldest once its limit is reached.\n",
        "\n",
        "This type of memory can be crucial in situations like:\n",
        "\n",
        "1. **Lengthy Dialogues**: During drawn-out conversations where only the most recent exchanges are relevant, using a ConversationBufferWindowMemory can prevent unnecessary use of memory resources.\n",
        "\n",
        "2. **Highly Contextual Exchanges**: If the AI needs to respond based on recent contextual information rather than older, potentially irrelevant facts, this memory type is beneficial.\n",
        "\n",
        "3. **Memory and Cost Efficiency**: As lengthy conversations can occupy large amounts of memory and increase the cost of processing tokens with an LLM, using a ConversationBufferWindowMemory can help control costs by reducing memory use.\n",
        "\n",
        "For instance, in certain customer support scenarios, an AI might need to keep track of just the last few exchanges to handle the current query, making the ConversationBufferWindowMemory a fitting option."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DVLouEew-ppC"
      },
      "source": [
        "## ConversationTokenBufferMemory\n",
        "\n",
        "This memory limits the number of tokens stored. It's useful because a large portion of the cost in language model usage comes from the number of tokens processed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "muIbTkP9-ppC",
        "outputId": "4f33641c-c85b-4f15-ada5-e7a8aae95ae6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.8 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.3/1.8 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.8/1.8 MB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install tiktoken -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v7cAglLV-ppD"
      },
      "outputs": [],
      "source": [
        "from langchain.memory import ConversationTokenBufferMemory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MGdAkQMQ-ppD"
      },
      "outputs": [],
      "source": [
        "memory = ConversationTokenBufferMemory(llm=llm, max_token_limit=50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rOatQfK1-ppD"
      },
      "outputs": [],
      "source": [
        "memory.save_context({\"input\": \"Embeddings are what?\"},\n",
        "                    {\"output\": \"Exciting!\"})\n",
        "\n",
        "memory.save_context({\"input\": \"AI is what?!\"},\n",
        "                    {\"output\": \"Amazing!\"})\n",
        "\n",
        "memory.save_context({\"input\": \"Backpropagation is what?\"},\n",
        "                    {\"output\": \"Beautiful!\"})\n",
        "\n",
        "memory.save_context({\"input\": \"Deep learning is what?\"},\n",
        "                    {\"output\": \"Delightful!\"})\n",
        "\n",
        "memory.save_context({\"input\": \"Chatbots are what?\"},\n",
        "                    {\"output\": \"Charming!\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GQH5vw-e-ppD",
        "outputId": "a22ecb32-f990-4510-b02e-1f83a9528e9c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': 'AI: Beautiful!\\nHuman: Deep learning is what?\\nAI: Delightful!\\nHuman: Chatbots are what?\\nAI: Charming!'}"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "memory.load_memory_variables({})"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "memory.buffer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "5pXlGnNfYOMd",
        "outputId": "a386e236-3f88-4543-9765-04db4d69a3d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'AI: Beautiful!\\nHuman: Deep learning is what?\\nAI: Delightful!\\nHuman: Chatbots are what?\\nAI: Charming!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BGfoLvH6-ppD"
      },
      "source": [
        "The **ConversationTokenBufferMemory** is used when it's crucial to control both memory usage and the cost associated with a language model's token processing. This type of memory can limit the number of tokens stored to a defined maximum, which directly maps to the cost of the language model calls.\n",
        "\n",
        "Here are some examples where ConversationTokenBufferMemory can be beneficial:\n",
        "\n",
        "1. **Cost-Effective Chatbots**: For interactive chatbots being used extensively, limiting the number of tokens processed can help reduce operation costs associated with large language models, particularly when a user interacts frequently or provides extensive inputs.\n",
        "\n",
        "2. **Large-Scale Applications**: In large-scale applications that handle multiple instances of language models for multiple users, using ConversationTokenBufferMemory could improve performance while remaining cost-effective.\n",
        "\n",
        "3. **Data Capping**: When you want to put a cap on the amount of data stored and processed for practical or regulatory reasons, the token buffer memory allows this flexibility.\n",
        "\n",
        "In essence, wherever controlling the amount of processed data or tokens is crucial for performance, cost, or compliance reasons, ConversationTokenBufferMemory provides an effective solution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25e2bn_k-ppE"
      },
      "source": [
        "## ConversationSummaryBufferMemory\n",
        "\n",
        "Finally, the ConversationSummaryBufferMemory uses a language model to summarize the conversation when the token limit is reached. For example, if you have a long conversation and you want to summarize it, this memory type can be helpful."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nzesx76g-ppE"
      },
      "outputs": [],
      "source": [
        "from langchain.memory import ConversationSummaryBufferMemory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DEy0hxp7-ppE"
      },
      "outputs": [],
      "source": [
        "scheduleString = \"This is your personalized and detailed daily schedule reminder. You begin your day bright and early at 6 am with a few minutes of mindfulness and meditation to set the tone for the day. At 7 am, you have a nutritious breakfast, followed by a re-energizing morning workout at 7:30 am. After freshening up, you dive into your first meeting of the day at 9 am that continues until 10 am. Subsequently, from 10:30 am to 12 noon, you'll be conducting a project review. When lunchtime rolls around at 12 pm, take a well-deserved break until 1 pm. Once you're rejuvenated, you're set to join a brainstorming session from 1:30 pm to 3 pm. You've set aside the time between 3 pm to 4 pm to catch up on emails and pending tasks. From 4:30 pm to 5:30 pm, you're scheduled for another meeting. Afterwards, from 5:30 pm onwards, it's your leisure time to be used as you see fit. Remember to have your dinner at 7:30 pm to keep you fueled for the next day. Before retiring for the night at 10 pm, allow yourself some relaxation or quiet reading time. Throughout the day, it's critically important to stay hydrated and take short breaks for proper rest. Here's to a productive and successful day ahead!\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PiGLMBD2-ppM"
      },
      "outputs": [],
      "source": [
        "memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=20)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OuM4Zx-j-ppM"
      },
      "outputs": [],
      "source": [
        "memory.save_context({\"input\": \"Hello\"}, {\"output\": \"What's up\"})\n",
        "\n",
        "memory.save_context({\"input\": \"Not much, just hanging\"},\n",
        "                    {\"output\": \"Cool\"})\n",
        "\n",
        "memory.save_context({\"input\": \"What is on the schedule today?\"},\n",
        "                    {\"output\": f\"{scheduleString}\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Ns16rjC-ppM",
        "outputId": "1440afad-f616-4c68-c4ad-b3ac3a131a3e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': \"System: The human greets the AI and casually mentions they're just hanging out. When asked about the day's schedule, the AI provides a detailed itinerary starting with mindfulness and meditation at 6 am, followed by breakfast and a workout. The day continues with meetings, a project review, a brainstorming session, and time for emails and pending tasks. The evening is left open for leisure activities, with reminders to eat dinner and relax before bed. The AI emphasizes the importance of staying hydrated and taking breaks throughout the day.\"}"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ],
      "source": [
        "memory.load_memory_variables({})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_T0MafJQ-ppN"
      },
      "outputs": [],
      "source": [
        "conversation = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory = memory,\n",
        "    verbose=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZfyEij4m-ppN",
        "outputId": "0a689b02-42dd-4700-cd7a-658eeb8cf5ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "System: The human greets the AI and asks about their schedule for the day. The AI provides a detailed schedule, starting with mindfulness and meditation at 6 am, followed by breakfast and a workout. The day continues with a series of meetings, a project review, a brainstorming session, and time for catching up on emails and tasks. The evening is set aside for leisure activities, dinner, and relaxation before bed at 10 pm. The AI emphasizes the importance of staying hydrated and taking breaks throughout the day.\n",
            "Human: When the first meeting?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"As an AI, I don't actually have physical meetings. However, in the context of the schedule I provided, let's say the first meeting would be around 9 am, after breakfast and a workout.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 39
        }
      ],
      "source": [
        "conversation.predict(input=\"When the first meeting?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TObhgM1v-ppN",
        "outputId": "3b23507a-1f79-41f5-8881-d6aba86134a9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': \"System: The human greets the AI and asks about their schedule for the day. The AI provides a detailed schedule, starting with mindfulness and meditation at 6 am, followed by breakfast and a workout. The day continues with a series of meetings, a project review, a brainstorming session, and time for catching up on emails and tasks. The evening is set aside for leisure activities, dinner, and relaxation before bed at 10 pm. The AI emphasizes the importance of staying hydrated and taking breaks throughout the day.\\nHuman: When the first meeting?\\nAI: As an AI, I don't actually have physical meetings. However, in the context of the schedule I provided, let's say the first meeting would be around 9 am, after breakfast and a workout.\"}"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ],
      "source": [
        "memory.load_memory_variables({})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Up4vRzET-ppN"
      },
      "source": [
        "**ConversationSummaryBufferMemory** is employed in situations where you'd like to maintain the gist of the conversation without storing every single detail because of token or resource limitations. It uses a language model to summarize the dialogue and stores the summary.\n",
        "\n",
        "Here are a few typical instances where you might find this memory type useful:\n",
        "\n",
        "1. **Large-scale Conversations**: If a conversation is extensive or spans over a long period of time and storing every detail isn't feasible due to token limitations or efficiency concerns, ConversationSummaryBufferMemory would be useful for retaining the most critical information without exhausting the resources.\n",
        "\n",
        "2. **Business Meetings**: In digital platforms for brainstorming, meetings, or webinars, we often need to create a concise summary of the long conversations. This memory type could be very effective in such contexts.\n",
        "\n",
        "3. **Educational Platforms**: In an e-learning environment, where a teacher interacts with several students, ConversationSummaryBufferMemory could assist in summarizing the key points of this interaction, aiding in generating reviewable content after a class or a session.\n",
        "\n",
        "4. **News or Document Summarizing Applications**: If a conversation or text represents a news article or a document that needs to be summarized, ConversationSummaryBufferMemory could store the summarised version, helping users get the crux of the information promptly.\n",
        "\n",
        "In essence, ConversationSummaryBufferMemory enables you to retain the core thread of a dialogue or content even with a high volume of tokens, offering a summary of the main points while using fewer resources.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trojkL6Q-ppO"
      },
      "source": [
        "In addition to what has been mentioned, LangChain also supports two other features: `VectorStoreRetrieverMemory`, which stores word or text embeddings, and `ConversationEntityMemory`, which retains details about specific entities.\n",
        "\n",
        "These two features will be explained in detail in the subsequent material.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mss3TqTS-ppO"
      },
      "source": [
        "## Memory Exercise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FIilSDFL-ppO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c201ab94-9e4a-4f64-cde5-5a952632d4a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rggrader\n",
            "  Downloading rggrader-0.1.6-py3-none-any.whl (2.5 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from rggrader) (2.31.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from rggrader) (1.5.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from rggrader) (9.4.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->rggrader) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->rggrader) (2023.4)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas->rggrader) (1.25.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->rggrader) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->rggrader) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->rggrader) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->rggrader) (2024.2.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->rggrader) (1.16.0)\n",
            "Installing collected packages: rggrader\n",
            "Successfully installed rggrader-0.1.6\n"
          ]
        }
      ],
      "source": [
        "%pip install rggrader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h4DybVPs-ppO"
      },
      "outputs": [],
      "source": [
        "# @title #### Student Identity\n",
        "student_id = \"your student id\" # @param {type:\"string\"}\n",
        "name = \"your name\" # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nt8qp1dz-ppO"
      },
      "outputs": [],
      "source": [
        "# @title #### 00. Router Chain\n",
        "from rggrader import submit\n",
        "import os\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "from langchain.chains import ConversationChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "# TODO:\n",
        "# Create a chat application using the `ConversationBufferMemory` memory type\n",
        "# Provide `ConversationBufferMemory` in the `memory' variable which will store data across calls to the chain.\n",
        "# Also use the GPT 3.5-turbo chat model in the process.\n",
        "# Have several conversations to see whether your chat application can remember/recognize the information previously asked.\n",
        "\n",
        "# Put your code here:\n",
        "memory = ConversationBufferMemory()\n",
        "\n",
        "question=\"your question\" # @param {type:\"string\"}\n",
        "conversationUsingBufferMemory.predict(input=question)\n",
        "# ---- End of your code ----\n",
        "\n",
        "# Example of Expected Output:\n",
        "\"\"\"\n",
        "> Entering new ConversationChain chain...\n",
        "Prompt after formatting:\n",
        "The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
        "\n",
        "Current conversation:\n",
        "Human: Hello, i'm Djarot Purnomo. I'm a UI/UX Designer\n",
        "AI: Hello Djarot Purnomo! It's nice to meet you. As a UI/UX Designer, you must have a lot of experience in creating user-friendly and visually appealing interfaces. How long have you been working in this field?\n",
        "Human: explain the relationship between a UI/UX Designer and a Front End Engineer\n",
        "AI: A UI/UX Designer and a Front End Engineer often work closely together to create a seamless user experience. The UI/UX Designer focuses on the overall look and feel of the interface, including the layout, color scheme, and typography. They also conduct user research and create wireframes and prototypes to test and refine the design.\n",
        "\n",
        "On the other hand, the Front End Engineer is responsible for implementing the design into a functional website or application. They take the visual elements created by the UI/UX Designer and use programming languages like HTML, CSS, and JavaScript to bring them to life. They ensure that the design is responsive, interactive, and optimized for different devices and browsers.\n",
        "\n",
        "In summary, the UI/UX Designer focuses on the user experience and visual design, while the Front End Engineer focuses on the technical implementation of that design. They collaborate closely to ensure that the final product is both visually appealing and user-friendly.\n",
        "Human: What skills do I currently have?\n",
        "AI:\n",
        "\n",
        "> Finished chain.\n",
        "As an AI, I don't have access to personal information or knowledge about your specific skills. Therefore, I cannot accurately determine what skills you currently have. However, based on your previous statement that you are a UI/UX Designer, it can be inferred that you have skills in creating user-friendly interfaces, conducting user research, creating wireframes and prototypes, and possibly knowledge of design tools such as Adobe XD or Sketch.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "foxPYuaO-ppP"
      },
      "outputs": [],
      "source": [
        "# Submit Method\n",
        "assignment_id = \"00_memory\"\n",
        "question_id = \"00_conversation_buffer_memory\"\n",
        "history = memory.buffer\n",
        "submit(student_id, name, assignment_id, history, question_id)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}