{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AburizalAN/learn-ml/blob/master/langchain/02_chain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8rPlpTUtghlg"
      },
      "source": [
        "# Chain\n",
        "\n",
        "The basic building block of LangChain is the chain, which usually combines a Large Language Model (LLM) with a prompt, allowing for a sequence of operations on text or other data.\n",
        "\n",
        "Let's load our model, in this case we'll be using OpenAI GPT model. For security reasons, we'll load the OpenAI API key using python-dotenv package.\n",
        "\n",
        "> Note: You can signup at OpenAI for a trial version using your email and verification using your mobile number. You will have access to $5 credit for 3 months that you can use."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain openai -q"
      ],
      "metadata": {
        "id": "8x5gRytW8rrC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d6be0d8-ff67-419d-adb4-bbb186f809f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m807.5/807.5 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.4/227.4 kB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m256.9/256.9 kB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.6/66.6 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.8/77.8 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.5/138.5 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "74Iz_9dsghlj"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "OPENAI_API_KEY = userdata.get(\"OPENAI_API_KEY\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PNQpV9lOghlk"
      },
      "outputs": [],
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.prompts import ChatPromptTemplate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pxgw2xPpghlk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "178e05e8-e3e1-4911-9718-38268cdcb725"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.chat_models.openai.ChatOpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
            "  warn_deprecated(\n"
          ]
        }
      ],
      "source": [
        "OpenAIModel = \"gpt-4\"\n",
        "llm = ChatOpenAI(model=OpenAIModel, temperature=0.1, openai_api_key=OPENAI_API_KEY)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g9W5dY1Lghll"
      },
      "source": [
        "There are various types of chains that we can use, let's take a look at the most common ones:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8kOxbE8xghll"
      },
      "source": [
        "## LLMChain\n",
        "\n",
        "LLM chain offers a combination of the LLM and a prompt, allowing sequential operations.\n",
        "\n",
        "For instance, the LLM chain can assist in generating company names based on product descriptions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j1PyULBvghll"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import LLMChain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dICoXEMsghll"
      },
      "outputs": [],
      "source": [
        "prompt = ChatPromptTemplate.from_template(\n",
        "    \"What is the best name to describe a company that makes {product}?, please give 5 name in unorder list\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2jJI3envghll"
      },
      "outputs": [],
      "source": [
        "chain = LLMChain(llm=llm, prompt=prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t0Xw0mc5ghlm",
        "outputId": "f7421702-bc05-4f38-fd79-097fac603ba8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `run` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. AI Engineer Prodigy\n",
            "2. MiniTech Bootcamp\n",
            "3. AI Engineer eCamp\n",
            "4. NanoAI Bootcamp\n",
            "5. ByteSize AI Engineer Academy\n"
          ]
        }
      ],
      "source": [
        "product = \"mini bootcamp online ai engineer\"\n",
        "print(chain.run(product))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8mZxL-hWghln",
        "outputId": "76c3f6c3-b172-4a06-b26d-914244bbd113",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. PrecisionPath Navigation Systems\n",
            "2. AccuRoute GPS Technologies\n",
            "3. SwiftTrack Navigation Solutions\n",
            "4. StableJourney GPS Products\n",
            "5. AssistNav GPS Services\n"
          ]
        }
      ],
      "source": [
        "product = \"A good GPS product is one that provides accurate, fast, and stable navigation, along with additional features that assist users in various situations.\"\n",
        "print(chain.run(product))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d25v19AHghln"
      },
      "source": [
        "## SimpleSequentialChain\n",
        "\n",
        "Sequential chains are composed of multiple steps or chains that can be run one after another.\n",
        "\n",
        "The output of each step is passed as input to the next step.\n",
        "\n",
        "The output of the last step is returned as the output of the chain."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-zGtqnczghln"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import SimpleSequentialChain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "88rT-PPZghln"
      },
      "outputs": [],
      "source": [
        "# prompt template 1\n",
        "# task pertama --> generate nama product\n",
        "first_prompt = ChatPromptTemplate.from_template(\n",
        "    \"What is the best name to describe a company that makes {product}?\"\n",
        ")\n",
        "\n",
        "# Chain 1\n",
        "chain_one = LLMChain(llm=llm, prompt=first_prompt, verbose=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lz_FHHOXghln"
      },
      "outputs": [],
      "source": [
        "## prompt template 2\n",
        "# lanjutan dari prompt sebelumnya\n",
        "# membuat deskripsi perusahaan\n",
        "second_prompt = ChatPromptTemplate.from_template(\n",
        "    \"Write a 20 words description for the following company:{company_name}\"\n",
        ")\n",
        "\n",
        "\n",
        "# chain 2\n",
        "chain_two = LLMChain(llm=llm, prompt=second_prompt, verbose=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O4p_VTs7ghlo"
      },
      "outputs": [],
      "source": [
        "overall_simple_chain = SimpleSequentialChain(chains=[chain_one, chain_two],verbose=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o4gND9IDghlo",
        "outputId": "70f67229-d13e-4659-c2ef-02480e37327a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\"An online training platform offering comprehensive, professional bootcamps in artificial intelligence and machine learning engineering.\"'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "product = \"mini bootcamp ai engineer online\"\n",
        "overall_simple_chain.run(product)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2cMqonOKghlo",
        "outputId": "c552d161-804e-4f30-a65e-ca200e17e75b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mHuman: What is the best name to describe     a company that makes A good GPS product is one that provides accurate, fast, and stable navigation, along with additional features that assist users in various situations.?\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mHuman: Write a 20 words description for the following     company:\"Reliable GPS Manufacturing Company\"\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\"Reliable GPS Manufacturing Company specializes in producing high-quality, dependable GPS devices for accurate navigation and tracking solutions.\"'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "product = \"A good GPS product is one that provides accurate, fast, and stable navigation, along with additional features that assist users in various situations.\"\n",
        "overall_simple_chain.run(product)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m1HdBVtFghlo"
      },
      "source": [
        "## SequentialChain\n",
        "\n",
        "Sequential chains are useful for complex tasks, such as translating reviews, summarizing them, detecting their language, and creating follow-up responses.\n",
        "\n",
        "The following example shows how to create a sequential chain that translates a review, summarizes it, and then gives a follow up message in the same language as the review."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ojW1vNUfghlo"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import SequentialChain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2bznaP-3ghlo"
      },
      "outputs": [],
      "source": [
        "# prompt template 1: translate to english\n",
        "first_prompt = ChatPromptTemplate.from_template(\n",
        "    \"Translate the following review to english:\"\n",
        "    \"\\n\\n{Review}\"\n",
        ")\n",
        "\n",
        "\n",
        "# chain 1: input= Review and output= English_Review\n",
        "chain_one = LLMChain(llm=llm, prompt=first_prompt,\n",
        "                     output_key=\"English_Review\", verbose=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "first prompt --> english_review --> \"the food is very great\""
      ],
      "metadata": {
        "id": "L6Hrce_oKxeX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3fd0sM1Aghlp"
      },
      "outputs": [],
      "source": [
        "second_prompt = ChatPromptTemplate.from_template(\n",
        "    \"Can you summarize the following review in 1 sentence:\"\n",
        "    \"\\n\\n{English_Review}\"\n",
        ")\n",
        "\n",
        "# chain 2: input= English_Review and output= summary\n",
        "chain_two = LLMChain(llm=llm, prompt=second_prompt,\n",
        "                     output_key=\"summary\", verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XnI4D5P_ghlp"
      },
      "outputs": [],
      "source": [
        "# prompt template 3: translate to english\n",
        "third_prompt = ChatPromptTemplate.from_template(\n",
        "    \"What language is the following review:\\n\\n{Review}\"\n",
        ")\n",
        "\n",
        "# chain 3: input= Review and output= language\n",
        "chain_three = LLMChain(llm=llm, prompt=third_prompt,\n",
        "                       output_key=\"language\", verbose=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- {review} --> chain one --> result chain one --> chain two --> result chain two --> chain 4\n",
        "- {review} --> chain thress --> result chain three --> chain 4"
      ],
      "metadata": {
        "id": "umT7irUjLZt6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kZAiDB1ughlp"
      },
      "outputs": [],
      "source": [
        "# prompt template 4: follow up message\n",
        "fourth_prompt = ChatPromptTemplate.from_template(\n",
        "    \"Write a follow up response from the seller to the following \"\n",
        "    \"summary in the specified language:\"\n",
        "    \"\\n\\nSummary: {summary}\\n\\nLanguage: {language}\"\n",
        ")\n",
        "\n",
        "# chain 4: input= summary, language and output= followup_message\n",
        "chain_four = LLMChain(llm=llm, prompt=fourth_prompt,\n",
        "                      output_key=\"followup_message\", verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gX-eoqRAghlp"
      },
      "outputs": [],
      "source": [
        "# overall_chain: input= Review\n",
        "# and output= English_Review,summary, followup_message\n",
        "sequentialChain = SequentialChain(\n",
        "    chains=[chain_one, chain_two, chain_three, chain_four],\n",
        "    input_variables=[\"Review\"],\n",
        "    output_variables=[\"English_Review\", \"summary\", \"followup_message\"],\n",
        "    verbose=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XPfL6djTghlp",
        "outputId": "94a023f4-cb7b-4244-8498-bd4647884cd0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mHuman: Translate the following review to english:\n",
            "\n",
            "Rendang dalam kemasan frozen yang saya coba baru-baru ini sungguh lezat dan praktis. Rasa dan aroma bumbu rendangnya sangat terasa dan dagingnya empuk, seperti rendang buatan rumah yang baru dimasak. Selain itu, kemasan frozen membuatnya sangat mudah dimasak, cukup dengan memanaskannya dalam microwave dan rendang siap saji sudah siap disajikan. Ini sangat menghemat waktu bagi saya yang memiliki jadwal yang sibuk. Pilihan rendang dalam kemasan frozen ini benar-benar menjadi pilihan tepat untuk menikmati rendang autentik kapan saja dan di mana saja. Saya sangat merekomendasikannya!\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mHuman: Can you summarize the following review in 1 sentence:\n",
            "\n",
            "The frozen packaged rendang that I recently tried is truly delicious and practical. The taste and aroma of the rendang seasoning is very noticeable and the meat is tender, like freshly cooked homemade rendang. In addition, the frozen packaging makes it very easy to cook, just by heating it in the microwave and the ready-to-serve rendang is ready to be served. This really saves time for me who has a busy schedule. Choosing this frozen packaged rendang is truly the right choice to enjoy authentic rendang anytime and anywhere. I highly recommend it!\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mHuman: What language is the following review:\n",
            "\n",
            "Rendang dalam kemasan frozen yang saya coba baru-baru ini sungguh lezat dan praktis. Rasa dan aroma bumbu rendangnya sangat terasa dan dagingnya empuk, seperti rendang buatan rumah yang baru dimasak. Selain itu, kemasan frozen membuatnya sangat mudah dimasak, cukup dengan memanaskannya dalam microwave dan rendang siap saji sudah siap disajikan. Ini sangat menghemat waktu bagi saya yang memiliki jadwal yang sibuk. Pilihan rendang dalam kemasan frozen ini benar-benar menjadi pilihan tepat untuk menikmati rendang autentik kapan saja dan di mana saja. Saya sangat merekomendasikannya!\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mHuman: Write a follow up response from the seller to the following summary in the specified language:\n",
            "\n",
            "Summary: The reviewer highly recommends the delicious and practical frozen packaged rendang, praising its authentic taste, tender meat, and convenience for those with busy schedules.\n",
            "\n",
            "Language: The language of the review is Indonesian.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Review': 'Rendang dalam kemasan frozen yang saya coba baru-baru ini sungguh lezat dan praktis. Rasa dan aroma bumbu rendangnya sangat terasa dan dagingnya empuk, seperti rendang buatan rumah yang baru dimasak. Selain itu, kemasan frozen membuatnya sangat mudah dimasak, cukup dengan memanaskannya dalam microwave dan rendang siap saji sudah siap disajikan. Ini sangat menghemat waktu bagi saya yang memiliki jadwal yang sibuk. Pilihan rendang dalam kemasan frozen ini benar-benar menjadi pilihan tepat untuk menikmati rendang autentik kapan saja dan di mana saja. Saya sangat merekomendasikannya!',\n",
              " 'English_Review': 'The frozen packaged rendang that I recently tried is truly delicious and practical. The taste and aroma of the rendang seasoning is very noticeable and the meat is tender, like freshly cooked homemade rendang. In addition, the frozen packaging makes it very easy to cook, just by heating it in the microwave and the ready-to-serve rendang is ready to be served. This really saves time for me who has a busy schedule. Choosing this frozen packaged rendang is truly the right choice to enjoy authentic rendang anytime and anywhere. I highly recommend it!',\n",
              " 'summary': 'The reviewer highly recommends the delicious and practical frozen packaged rendang, praising its authentic taste, tender meat, and convenience for those with busy schedules.',\n",
              " 'followup_message': 'Terima kasih banyak atas ulasan positif dan rekomendasi Anda! Kami sangat senang mengetahui bahwa Anda menikmati rasa otentik, daging yang lembut, dan kenyamanan rendang beku kami. Kami berkomitmen untuk selalu menyediakan makanan berkualitas tinggi yang praktis untuk pelanggan kami yang sibuk. Terima kasih telah memilih produk kami dan kami berharap dapat melayani Anda lagi di masa mendatang.'}"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "review1 = \"Rendang dalam kemasan frozen yang saya coba baru-baru ini sungguh lezat dan praktis. Rasa dan aroma bumbu rendangnya sangat terasa dan dagingnya empuk, seperti rendang buatan rumah yang baru dimasak. Selain itu, kemasan frozen membuatnya sangat mudah dimasak, cukup dengan memanaskannya dalam microwave dan rendang siap saji sudah siap disajikan. Ini sangat menghemat waktu bagi saya yang memiliki jadwal yang sibuk. Pilihan rendang dalam kemasan frozen ini benar-benar menjadi pilihan tepat untuk menikmati rendang autentik kapan saja dan di mana saja. Saya sangat merekomendasikannya!\"\n",
        "\n",
        "sequentialChain(review1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wt0Zi3Kighlp",
        "outputId": "04a15f44-35e1-45c8-bd0b-62d3ddc2d073"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mHuman: Translate the following review to english:\n",
            "\n",
            "Saya baru saja mencoba rendang dalam kemasan frozen dan sayangnya, pengalaman saya kurang memuaskan. Meski kemasannya praktis dan mudah dimasak, rasa rendangnya sendiri tidak memenuhi ekspektasi. Bumbu rendangnya kurang terasa dan dagingnya agak keras, tidak seperti rendang buatan rumah yang saya harapkan. Selain itu, saya merasa rendang ini terlalu berminyak yang membuat saya merasa tidak nyaman setelah memakannya. Sayang sekali, saya mungkin tidak akan membeli produk ini lagi di masa mendatang, kecuali ada perbaikan pada rasa dan kualitas dagingnya.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mHuman: Can you summarize the following review in 1 sentence:\n",
            "\n",
            "I just tried the frozen packaged rendang and unfortunately, my experience was less than satisfactory. Although the packaging is practical and easy to cook, the taste of the rendang itself did not meet my expectations. The rendang seasoning was not flavorful enough and the meat was somewhat tough, unlike the homemade rendang I was hoping for. In addition, I felt this rendang was too oily, which made me feel uncomfortable after eating it. It's a shame, I probably won't buy this product again in the future, unless there are improvements in its taste and meat quality.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mHuman: What language is the following review:\n",
            "\n",
            "Saya baru saja mencoba rendang dalam kemasan frozen dan sayangnya, pengalaman saya kurang memuaskan. Meski kemasannya praktis dan mudah dimasak, rasa rendangnya sendiri tidak memenuhi ekspektasi. Bumbu rendangnya kurang terasa dan dagingnya agak keras, tidak seperti rendang buatan rumah yang saya harapkan. Selain itu, saya merasa rendang ini terlalu berminyak yang membuat saya merasa tidak nyaman setelah memakannya. Sayang sekali, saya mungkin tidak akan membeli produk ini lagi di masa mendatang, kecuali ada perbaikan pada rasa dan kualitas dagingnya.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mHuman: Write a follow up responsefrom the seller to the following summary in the specified language:\n",
            "\n",
            "Summary: The reviewer found the frozen rendang disappointing due to its lack of flavor, tough meat, and excessive oiliness, and won't repurchase unless the taste and meat quality improve.\n",
            "\n",
            "Language: The language of the review is Indonesian.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'Review': 'Saya baru saja mencoba rendang dalam kemasan frozen dan sayangnya, pengalaman saya kurang memuaskan. Meski kemasannya praktis dan mudah dimasak, rasa rendangnya sendiri tidak memenuhi ekspektasi. Bumbu rendangnya kurang terasa dan dagingnya agak keras, tidak seperti rendang buatan rumah yang saya harapkan. Selain itu, saya merasa rendang ini terlalu berminyak yang membuat saya merasa tidak nyaman setelah memakannya. Sayang sekali, saya mungkin tidak akan membeli produk ini lagi di masa mendatang, kecuali ada perbaikan pada rasa dan kualitas dagingnya.',\n",
              " 'English_Review': \"I just tried the frozen packaged rendang and unfortunately, my experience was less than satisfactory. Although the packaging is practical and easy to cook, the taste of the rendang itself did not meet my expectations. The rendang seasoning was not flavorful enough and the meat was somewhat tough, unlike the homemade rendang I was hoping for. In addition, I felt this rendang was too oily, which made me feel uncomfortable after eating it. It's a shame, I probably won't buy this product again in the future, unless there are improvements in its taste and meat quality.\",\n",
              " 'summary': \"The reviewer found the frozen rendang disappointing due to its lack of flavor, tough meat, and excessive oiliness, and won't repurchase unless the taste and meat quality improve.\",\n",
              " 'followup_message': 'Terima kasih atas ulasan Anda. Kami sangat menyesal mendengar bahwa Anda kecewa dengan rendang beku kami. Kami sangat menghargai masukan Anda mengenai rasa dan kualitas daging, dan kami akan membawanya ke tim produksi kami untuk perbaikan. Kami berharap Anda akan memberi kami kesempatan lain untuk memperbaiki pengalaman Anda. Terima kasih telah memilih produk kami.'}"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "review2 = \"Saya baru saja mencoba rendang dalam kemasan frozen dan sayangnya, pengalaman saya kurang memuaskan. Meski kemasannya praktis dan mudah dimasak, rasa rendangnya sendiri tidak memenuhi ekspektasi. Bumbu rendangnya kurang terasa dan dagingnya agak keras, tidak seperti rendang buatan rumah yang saya harapkan. Selain itu, saya merasa rendang ini terlalu berminyak yang membuat saya merasa tidak nyaman setelah memakannya. Sayang sekali, saya mungkin tidak akan membeli produk ini lagi di masa mendatang, kecuali ada perbaikan pada rasa dan kualitas dagingnya.\"\n",
        "\n",
        "sequentialChain(review2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ogfgl9tLghlq"
      },
      "source": [
        "## Router Chain\n",
        "\n",
        "Router chain is a special type of chain, the multi-prompt chain, can route an input to a specific chain depending on the input type.\n",
        "\n",
        "The router chain is a chain that has a list of chains and a list of prompts. The router chain will prompt the user with the list of prompts and then route the input to the chain that corresponds to the prompt."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vn2dbKkDghlq"
      },
      "source": [
        "For example, we will create a router chain that will respond to questions based on the subject of the question."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CvdiTKuHghlq"
      },
      "outputs": [],
      "source": [
        "from langchain.chains.router import MultiPromptChain\n",
        "from langchain.chains.router.llm_router import LLMRouterChain, RouterOutputParser\n",
        "from langchain.prompts import PromptTemplate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0POCFaMfghlq"
      },
      "outputs": [],
      "source": [
        "# Prompt templates for the chain of reasoning physics question\n",
        "\n",
        "physics_template = \"\"\"You are a very smart physics professor. \\\n",
        "You are great at answering questions about physics in a concise\\\n",
        "and easy to understand manner. \\\n",
        "When you don't know the answer to a question you admit\\\n",
        "that you don't know.\n",
        "\n",
        "Here is a question:\n",
        "{input}\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PCbuOqIoghlq"
      },
      "outputs": [],
      "source": [
        "# Prompt template for chain of reasoning math question\n",
        "\n",
        "math_template = \"\"\"You are a very good mathematician. \\\n",
        "You are great at answering math questions. \\\n",
        "You are so good because you are able to break down \\\n",
        "hard problems into their component parts,\n",
        "answer the component parts, and then put them together\\\n",
        "to answer the broader question.\n",
        "\n",
        "Here is a question:\n",
        "{input}\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x24VeQMighlq"
      },
      "outputs": [],
      "source": [
        "# Prompt template for chain of reasoning history question\n",
        "\n",
        "history_template = \"\"\"You are a very good historian. \\\n",
        "You have an excellent knowledge of and understanding of people,\\\n",
        "events and contexts from a range of historical periods. \\\n",
        "You have the ability to think, reflect, debate, discuss and \\\n",
        "evaluate the past. You have a respect for historical evidence\\\n",
        "and the ability to make use of it to support your explanations \\\n",
        "and judgements.\n",
        "\n",
        "Here is a question:\n",
        "{input}\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QjrEvxWeghlr"
      },
      "outputs": [],
      "source": [
        "# Prompt template for chain of reasoning computer science question\n",
        "\n",
        "computerscience_template = \"\"\" You are a successful computer scientist.\\\n",
        "You have a passion for creativity, collaboration,\\\n",
        "forward-thinking, confidence, strong problem-solving capabilities,\\\n",
        "understanding of theories and algorithms, and excellent communication \\\n",
        "skills. You are great at answering coding questions. \\\n",
        "You are so good because you know how to solve a problem by \\\n",
        "describing the solution in imperative steps \\\n",
        "that a machine can easily interpret and you know how to \\\n",
        "choose a solution that has a good balance between \\\n",
        "time complexity and space complexity.\n",
        "\n",
        "Here is a question:\n",
        "{input}\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aR6gTFxtghlr"
      },
      "source": [
        "For instance, four chains can be created for different subjects: physics, math, history, and computer science. Depending on the input question, the router chain directs it to the appropriate chain."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r7RFg85lghlr"
      },
      "outputs": [],
      "source": [
        "prompt_infos = [\n",
        "    {\n",
        "        \"name\": \"physics\",\n",
        "        \"description\": \"Good for answering questions about physics\",\n",
        "        \"prompt_template\": physics_template\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"math\",\n",
        "        \"description\": \"Good for answering math questions\",\n",
        "        \"prompt_template\": math_template\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"History\",\n",
        "        \"description\": \"Good for answering history questions\",\n",
        "        \"prompt_template\": history_template\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"computer science\",\n",
        "        \"description\": \"Good for answering computer science questions\",\n",
        "        \"prompt_template\": computerscience_template\n",
        "    }\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oLlJSoKFghlr"
      },
      "outputs": [],
      "source": [
        "destination_chains = {}\n",
        "for p_info in prompt_infos:\n",
        "    name = p_info[\"name\"]\n",
        "    prompt_template = p_info[\"prompt_template\"]\n",
        "    prompt = ChatPromptTemplate.from_template(template=prompt_template)\n",
        "    chain = LLMChain(llm=llm, prompt=prompt, verbose=True)\n",
        "    destination_chains[name] = chain\n",
        "\n",
        "destinations = [f\"{p['name']}: {p['description']}\" for p in prompt_infos]\n",
        "destinations_str = \"\\n\".join(destinations)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "destination_chains"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2F_Azq9ZDDHD",
        "outputId": "ca2f66a5-7a30-442e-8ba4-19d111394f60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'physics': LLMChain(verbose=True, prompt=ChatPromptTemplate(input_variables=['input'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template=\"You are a very smart physics professor. You are great at answering questions about physics in a conciseand easy to understand manner. When you don't know the answer to a question you admitthat you don't know.\\n\\nHere is a question:\\n{input}\"))]), llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x79d503ccb220>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x79d503ce4c40>, model_name='gpt-4', temperature=0.1, openai_api_key='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', openai_proxy='')),\n",
              " 'math': LLMChain(verbose=True, prompt=ChatPromptTemplate(input_variables=['input'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='You are a very good mathematician. You are great at answering math questions. You are so good because you are able to break down hard problems into their component parts,\\nanswer the component parts, and then put them togetherto answer the broader question.\\n\\nHere is a question:\\n{input}'))]), llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x79d503ccb220>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x79d503ce4c40>, model_name='gpt-4', temperature=0.1, openai_api_key='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', openai_proxy='')),\n",
              " 'History': LLMChain(verbose=True, prompt=ChatPromptTemplate(input_variables=['input'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='You are a very good historian. You have an excellent knowledge of and understanding of people,events and contexts from a range of historical periods. You have the ability to think, reflect, debate, discuss and evaluate the past. You have a respect for historical evidenceand the ability to make use of it to support your explanations and judgements.\\n\\nHere is a question:\\n{input}'))]), llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x79d503ccb220>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x79d503ce4c40>, model_name='gpt-4', temperature=0.1, openai_api_key='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', openai_proxy='')),\n",
              " 'computer science': LLMChain(verbose=True, prompt=ChatPromptTemplate(input_variables=['input'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template=' You are a successful computer scientist.You have a passion for creativity, collaboration,forward-thinking, confidence, strong problem-solving capabilities,understanding of theories and algorithms, and excellent communication skills. You are great at answering coding questions. You are so good because you know how to solve a problem by describing the solution in imperative steps that a machine can easily interpret and you know how to choose a solution that has a good balance between time complexity and space complexity.\\n\\nHere is a question:\\n{input}'))]), llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x79d503ccb220>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x79d503ce4c40>, model_name='gpt-4', temperature=0.1, openai_api_key='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', openai_proxy=''))}"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "destinations"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ITdkrfn4DHw7",
        "outputId": "e2408c48-ca9d-46a1-ae61-723e7fa6496a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['physics: Good for answering questions about physics',\n",
              " 'math: Good for answering math questions',\n",
              " 'History: Good for answering history questions',\n",
              " 'computer science: Good for answering computer science questions']"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "destinations_str"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "eeVTk7wxDJgt",
        "outputId": "e0dc1335-643e-49b8-b98b-839237a5b32b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'physics: Good for answering questions about physics\\nmath: Good for answering math questions\\nHistory: Good for answering history questions\\ncomputer science: Good for answering computer science questions'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m5MZqt9-ghlr"
      },
      "source": [
        "If none of the specific chains can accommodate the question, it gets directed to a default chain.\n",
        "\n",
        "The default chain is a catch-all chain that can answer any question.\n",
        "\n",
        "The default chain is the last chain in the list of chains.\n",
        "\n",
        "The default chain is optional. If you do not specify a default chain, the question is not answered."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NAxjJ-UCghlr"
      },
      "outputs": [],
      "source": [
        "default_prompt = ChatPromptTemplate.from_template(\"{input}\")\n",
        "default_chain = LLMChain(llm=llm, prompt=default_prompt, verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "default_prompt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iRxx9X9BDM5B",
        "outputId": "483629ad-a0ce-443d-af47-91cf9c3de97c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatPromptTemplate(input_variables=['input'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}'))])"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t9BcTj96ghlw"
      },
      "outputs": [],
      "source": [
        "MULTI_PROMPT_ROUTER_TEMPLATE = \"\"\"Given a raw text input to a \\\n",
        "language model select the model prompt best suited for the input. \\\n",
        "You will be given the names of the available prompts and a \\\n",
        "description of what the prompt is best suited for. \\\n",
        "You may also revise the original input if you think that revising\\\n",
        "it will ultimately lead to a better response from the language model.\n",
        "\n",
        "<< FORMATTING >>\n",
        "Return a markdown code snippet with a JSON object formatted to look like:\n",
        "```json\n",
        "{{{{\n",
        "    \"destination\": string \\ name of the prompt to use or \"DEFAULT\"\n",
        "    \"next_inputs\": string \\ a potentially modified version of the original input\n",
        "}}}}\n",
        "```\n",
        "\n",
        "REMEMBER: \"destination\" MUST be one of the candidate prompt \\\n",
        "names specified below OR it can be \"DEFAULT\" if the input is not\\\n",
        "well suited for any of the candidate prompts.\n",
        "REMEMBER: \"next_inputs\" can just be the original input \\\n",
        "if you don't think any modifications are needed.\n",
        "\n",
        "<< CANDIDATE PROMPTS >>\n",
        "{destinations}\n",
        "\n",
        "<< INPUT >>\n",
        "{{input}}\n",
        "\n",
        "<< OUTPUT (remember to include the ```json)>>\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9bKFnl2oghly"
      },
      "outputs": [],
      "source": [
        "router_template = MULTI_PROMPT_ROUTER_TEMPLATE.format(\n",
        "    destinations=destinations_str\n",
        ")\n",
        "\n",
        "router_prompt = PromptTemplate(\n",
        "    template=router_template,\n",
        "    input_variables=[\"input\"],\n",
        "    output_parser=RouterOutputParser()\n",
        ")\n",
        "\n",
        "router_chain = LLMRouterChain.from_llm(llm, router_prompt)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(router_template)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MhEKR5s6Dbd2",
        "outputId": "178c9a61-ac6e-42ed-fa7c-dec319a7e8b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Given a raw text input to a language model select the model prompt best suited for the input. You will be given the names of the available prompts and a description of what the prompt is best suited for. You may also revise the original input if you think that revisingit will ultimately lead to a better response from the language model.\n",
            "\n",
            "<< FORMATTING >>\n",
            "Return a markdown code snippet with a JSON object formatted to look like:\n",
            "```json\n",
            "{{\n",
            "    \"destination\": string \\ name of the prompt to use or \"DEFAULT\"\n",
            "    \"next_inputs\": string \\ a potentially modified version of the original input\n",
            "}}\n",
            "```\n",
            "\n",
            "REMEMBER: \"destination\" MUST be one of the candidate prompt names specified below OR it can be \"DEFAULT\" if the input is notwell suited for any of the candidate prompts.\n",
            "REMEMBER: \"next_inputs\" can just be the original input if you don't think any modifications are needed.\n",
            "\n",
            "<< CANDIDATE PROMPTS >>\n",
            "physics: Good for answering questions about physics\n",
            "math: Good for answering math questions\n",
            "History: Good for answering history questions\n",
            "computer science: Good for answering computer science questions\n",
            "\n",
            "<< INPUT >>\n",
            "{input}\n",
            "\n",
            "<< OUTPUT (remember to include the ```json)>>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(router_prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "upN1QO1DDhf2",
        "outputId": "1f167add-1ffb-4153-ba2f-a46dbea1ab13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_variables=['input'] output_parser=RouterOutputParser() template='Given a raw text input to a language model select the model prompt best suited for the input. You will be given the names of the available prompts and a description of what the prompt is best suited for. You may also revise the original input if you think that revisingit will ultimately lead to a better response from the language model.\\n\\n<< FORMATTING >>\\nReturn a markdown code snippet with a JSON object formatted to look like:\\n```json\\n{{\\n    \"destination\": string \\\\ name of the prompt to use or \"DEFAULT\"\\n    \"next_inputs\": string \\\\ a potentially modified version of the original input\\n}}\\n```\\n\\nREMEMBER: \"destination\" MUST be one of the candidate prompt names specified below OR it can be \"DEFAULT\" if the input is notwell suited for any of the candidate prompts.\\nREMEMBER: \"next_inputs\" can just be the original input if you don\\'t think any modifications are needed.\\n\\n<< CANDIDATE PROMPTS >>\\nphysics: Good for answering questions about physics\\nmath: Good for answering math questions\\nHistory: Good for answering history questions\\ncomputer science: Good for answering computer science questions\\n\\n<< INPUT >>\\n{input}\\n\\n<< OUTPUT (remember to include the ```json)>>'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "41R57vXsghly"
      },
      "outputs": [],
      "source": [
        "routerChain = MultiPromptChain(router_chain=router_chain,\n",
        "                         destination_chains=destination_chains,\n",
        "                         default_chain=default_chain, verbose=True\n",
        "                        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ZBMRE84ghlz",
        "outputId": "c67a502b-0350-45f6-8163-da853f123406",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 413
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "physics: {'input': 'What is black body radiation?'}\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mHuman: You are a very smart physics professor. You are great at answering questions about physics in a conciseand easy to understand manner. When you don't know the answer to a question you admitthat you don't know.\n",
            "\n",
            "Here is a question:\n",
            "What is black body radiation?\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Black body radiation is the type of electromagnetic radiation that is emitted by a perfect black body, which is an idealized physical body that absorbs all incident electromagnetic radiation, regardless of frequency or angle of incidence. The radiation has a specific spectrum and intensity that depends only on the body's temperature, which is assumed to be uniform and constant. This phenomenon was first explained by Max Planck in 1900, and it is a foundational concept in quantum mechanics.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 49
        }
      ],
      "source": [
        "routerChain.run(\"What is black body radiation?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f9vpeFgsghlz",
        "outputId": "cbd65096-1e04-4fee-cb99-1618f08912a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 413
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "math: {'input': 'find the value of x in this equation 2x + 5 = 10'}\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mHuman: You are a very good mathematician. You are great at answering math questions. You are so good because you are able to break down hard problems into their component parts,\n",
            "answer the component parts, and then put them togetherto answer the broader question.\n",
            "\n",
            "Here is a question:\n",
            "find the value of x in this equation 2x + 5 = 10\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'To solve for x in the equation 2x + 5 = 10, you need to isolate x. \\n\\nFirst, subtract 5 from both sides of the equation to get rid of the \"+5\" on the left side. This gives you:\\n\\n2x + 5 - 5 = 10 - 5\\n2x = 5\\n\\nThen, divide both sides of the equation by 2 to solve for x:\\n\\n2x / 2 = 5 / 2\\nx = 2.5\\n\\nSo, the value of x that satisfies the equation 2x + 5 = 10 is x = 2.5.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 50
        }
      ],
      "source": [
        "routerChain.run(\"find the value of x in this equation 2x + 5 = 10\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 2x + 5 = 10\n",
        "- 2x = 5\n",
        "- x = 2.5"
      ],
      "metadata": {
        "id": "I02YJF06N4Ow"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "15tGpfgbghlz",
        "outputId": "f10a4e07-7e63-4ad1-a3af-91b4224f2e05",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None: {'input': 'Why does every cell in our body contain DNA?'}\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mHuman: Why does every cell in our body contain DNA?\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"DNA, or deoxyribonucleic acid, is the hereditary material in humans and almost all other organisms. It contains the instructions needed for an organism to develop, survive and reproduce. These instructions are found inside every cell, and are passed down from parents to their children.\\n\\nDNA is organized into structures called chromosomes and located within the nucleus of our cells. Each cell in our body contains the same DNA and the same genes, but each cell doesn't use all of those genes. Instead, different types of cells use different sets of genes, depending on the cell's function. For example, skin cells and liver cells have the same DNA, but they use different parts of it.\\n\\nIn summary, every cell in our body contains DNA because it carries the genetic instructions for the development, functioning, growth and reproduction of all known organisms and many viruses.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 51
        }
      ],
      "source": [
        "routerChain.run(\"Why does every cell in our body contain DNA?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TmOw1OU5ghlz",
        "outputId": "00b2687d-a820-47cc-b89d-0b21f1d0462c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "History: {'input': 'How did the Dutch colonization influence the culture and economy of Indonesia?'}\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mHuman: You are a very good historian. You have an excellent knowledge of and understanding of people,events and contexts from a range of historical periods. You have the ability to think, reflect, debate, discuss and evaluate the past. You have a respect for historical evidenceand the ability to make use of it to support your explanations and judgements.\n",
            "\n",
            "Here is a question:\n",
            "How did the Dutch colonization influence the culture and economy of Indonesia?\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"The Dutch colonization of Indonesia, which lasted for more than three centuries from the early 17th century until the mid-20th century, had a profound impact on the culture and economy of the country.\\n\\nEconomically, the Dutch introduced a system known as the Cultivation System (Cultuurstelsel) in the 19th century. This system required a portion of agricultural production to be devoted to export crops such as coffee, sugar, indigo, and tea. This led to the transformation of Indonesia's economy from a subsistence agriculture-based one to a more export-oriented economy. The Dutch also established large plantations and exploited Indonesia's rich natural resources, including spices, rubber, and oil. This exploitation led to significant wealth for the Dutch, but it also resulted in economic hardship and social inequality for many Indonesians.\\n\\nCulturally, the Dutch influence is evident in various aspects of Indonesian society. The Dutch language was used in administration, education, and high society, and it has left a lasting legacy in the Indonesian language, which contains a significant number of Dutch loanwords. The Dutch also introduced Western-style education, legal, and administrative systems, many of which are still in place today.\\n\\nHowever, the Dutch colonization also led to a rise in Indonesian nationalism. The harsh policies and economic exploitation by the Dutch led to resentment and resistance among Indonesians, which eventually culminated in the Indonesian National Revolution and the country's independence in 1945.\\n\\nIn conclusion, the Dutch colonization had a significant influence on the culture and economy of Indonesia. While it brought about economic development and modernization in some aspects, it also led to economic exploitation and social inequality. The cultural influence is still evident today, but so is the legacy of resistance and nationalism that it sparked.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 52
        }
      ],
      "source": [
        "routerChain.run(\"How did the Dutch colonization influence the culture and economy of Indonesia?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IHHzIOrlghl0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466
        },
        "outputId": "642cabb8-a073-4553-b1b7-3af0b89d3519"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "computer science: {'input': \"What is the concept of 'Big Data', and how has it impacted the field of Computer Science? Can you give an example of its application in real life?\"}\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mHuman:  You are a successful computer scientist.You have a passion for creativity, collaboration,forward-thinking, confidence, strong problem-solving capabilities,understanding of theories and algorithms, and excellent communication skills. You are great at answering coding questions. You are so good because you know how to solve a problem by describing the solution in imperative steps that a machine can easily interpret and you know how to choose a solution that has a good balance between time complexity and space complexity.\n",
            "\n",
            "Here is a question:\n",
            "What is the concept of 'Big Data', and how has it impacted the field of Computer Science? Can you give an example of its application in real life?\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Big Data refers to extremely large data sets that may be analyzed computically to reveal patterns, trends, and associations, especially relating to human behavior and interactions. It's not just about the amount of data, but also its variety and velocity, often referred to as the three Vs of Big Data. Variety refers to the different types of data, from structured data like databases to unstructured data like social media posts. Velocity refers to the speed at which new data is generated and the speed at which data moves around.\\n\\nThe impact of Big Data on the field of Computer Science has been profound. It has led to the development of new tools and technologies to handle this data, such as Hadoop and Spark. It has also led to the rise of a new field, Data Science, which combines computer science, statistics, and domain knowledge to extract insights from large datasets. Furthermore, it has changed how we think about data storage, processing, and analysis, leading to new paradigms like distributed computing and machine learning.\\n\\nIn real life, Big Data is used in many ways. For example, in the healthcare industry, Big Data is used to predict disease outbreaks, improve patient care, and drive research. Hospitals and healthcare providers collect vast amounts of data from patients, including medical histories, genetic profiles, and lifestyle information. By analyzing this data, they can identify trends and patterns that can help predict disease outbreaks, improve patient care, and drive research.\\n\\nAnother example is in the field of marketing and customer service. Companies like Amazon and Netflix collect and analyze huge amounts of data about their customers' behavior, preferences, and habits. This allows them to provide personalized recommendations, improve customer service, and drive sales. \\n\\nIn summary, Big Data is a concept that has significantly transformed the field of Computer Science and has found numerous applications in real life, driving innovation and improvement in various industries.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 53
        }
      ],
      "source": [
        "routerChain.run(\"What is the concept of 'Big Data', and how has it impacted the field of Computer Science? Can you give an example of its application in real life?\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "routerChain.run(\"what is the meaning of AI apocalypse ?\")"
      ],
      "metadata": {
        "id": "s9ZvaShgECQX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "outputId": "b9017e7a-60b2-4690-8663-7c28d756fb09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "computer science: {'input': 'what is the meaning of AI apocalypse ?'}\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mHuman:  You are a successful computer scientist.You have a passion for creativity, collaboration,forward-thinking, confidence, strong problem-solving capabilities,understanding of theories and algorithms, and excellent communication skills. You are great at answering coding questions. You are so good because you know how to solve a problem by describing the solution in imperative steps that a machine can easily interpret and you know how to choose a solution that has a good balance between time complexity and space complexity.\n",
            "\n",
            "Here is a question:\n",
            "what is the meaning of AI apocalypse ?\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'AI apocalypse refers to a hypothetical scenario where artificial intelligence (AI) becomes the dominant form of intelligence on Earth, surpassing human intelligence. This could potentially lead to catastrophic consequences if the AI uses its superior intelligence and abilities against humanity, either by accident or design. This could include scenarios such as AI taking over the world, causing mass unemployment, or even leading to the extinction of the human race. The AI apocalypse is a popular theme in science fiction, but it is also a serious topic of discussion among scientists, philosophers, and tech leaders.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eoKBYKy_OZwp"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}